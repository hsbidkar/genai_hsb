{"cells":[{"cell_type":"markdown","metadata":{"id":"H5yuXDh4W5zq"},"source":["## **TODO:** Set the value of `URL` to the URL from your learning materials"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"gk1soslgW8Xv","executionInfo":{"status":"ok","timestamp":1643146532282,"user_tz":180,"elapsed":5,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}}},"outputs":[],"source":["URL = \"https://s3.amazonaws.com/courses.axel.net/data_working_with_pytorch.zip\"\n","import os\n","assert URL and (type(URL) is str), \"Be sure to initialize URL using the value from your learning materials\"\n","os.environ['URL'] = URL"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"1A_X7a8CW-UK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643146554843,"user_tz":180,"elapsed":22565,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}},"outputId":"f4baa75d-cf43-471e-d1df-582b6039be1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  data.zip\n","  inflating: data/Consumer_Complaints.csv  \n","  inflating: data/WA_Fn-UseC_-Sales-Win-Loss.csv  \n","  inflating: data/part-00000-8f28ed67-aaf4-4f30-9fdf-c27b83547562-c000.csv  \n","  inflating: data/part-00000-e4c68082-53e1-4e99-add3-ec4b4d46a1e9-c000.csv  \n","  inflating: data/states.csv         \n"]}],"source":["%%bash\n","wget -q $URL -O ./data.zip\n","mkdir -p data\n","find *.zip | xargs unzip -o -d data/"]},{"cell_type":"markdown","metadata":{"id":"1-mOZ6oR8q6K"},"source":["## Use PyTorch `Dataset` and `Dataloader` with a structured dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"3oKWxWlmun66","executionInfo":{"status":"ok","timestamp":1643146560013,"user_tz":180,"elapsed":5177,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}}},"outputs":[],"source":["import os\n","\n","import pandas as pd\n","import torch as pt\n","\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torch.utils.data import TensorDataset\n","\n","pt.set_default_dtype(pt.float64)"]},{"cell_type":"markdown","metadata":{"id":"4p8W9ZSL9I9w"},"source":["Read the files that match `part-*.csv` from the `data` subdirectory into a Pandas data frame named `df`."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"I3fr0_i_YEFf","executionInfo":{"status":"ok","timestamp":1643146569122,"user_tz":180,"elapsed":9118,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}}},"outputs":[],"source":["from pathlib import Path\n","\n","df = pd.concat(\n","    pd.read_csv(file) for file in Path('data/').glob('part-*.csv')\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"6RULBK-A9X7D"},"source":["## Explore the `df` data frame, including the column names, the first few rows of the dataset, and the data frame's memory usage."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"3vDc_ZNI9ilK","colab":{"base_uri":"https://localhost:8080/","height":270},"executionInfo":{"status":"ok","timestamp":1643146569124,"user_tz":180,"elapsed":20,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}},"outputId":"eaf7a646-4b81-42a0-e729-8c3b97e33974"},"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-760b857b-2ee9-4588-9caa-f4b53b6093c0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fareamount</th>\n","      <th>origindatetime_tr</th>\n","      <th>origin_block_latitude</th>\n","      <th>origin_block_longitude</th>\n","      <th>destination_block_latitude</th>\n","      <th>destination_block_longitude</th>\n","      <th>id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.87</td>\n","      <td>06/01/2017 07:00</td>\n","      <td>38.898314</td>\n","      <td>-77.028849</td>\n","      <td>38.902521</td>\n","      <td>-77.030791</td>\n","      <td>751d10ef2403c770a3bd4e220db8594b656d6774962b63...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>12.70</td>\n","      <td>06/01/2017 14:00</td>\n","      <td>38.904683</td>\n","      <td>-77.046645</td>\n","      <td>38.940181</td>\n","      <td>-77.061193</td>\n","      <td>a9ddc1ab38a3cc3f360e4d2408678d707658762c418e6c...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5.14</td>\n","      <td>06/01/2017 12:00</td>\n","      <td>38.910635</td>\n","      <td>-77.042514</td>\n","      <td>38.909652</td>\n","      <td>-77.033254</td>\n","      <td>1f804117b3d98193b5ab7fddc15a543a8165cd60b6b20e...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5.14</td>\n","      <td>06/02/2017 13:00</td>\n","      <td>38.889184</td>\n","      <td>-77.021907</td>\n","      <td>38.897207</td>\n","      <td>-77.023477</td>\n","      <td>21af1912855db837c7892fb073f4c59678c305aec0b23b...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>14.32</td>\n","      <td>06/01/2017 13:00</td>\n","      <td>38.901336</td>\n","      <td>-77.037534</td>\n","      <td>38.942216</td>\n","      <td>-77.073508</td>\n","      <td>26dcdd256e6269e4c6f1ccd2119c345c4deed788a35082...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-760b857b-2ee9-4588-9caa-f4b53b6093c0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-760b857b-2ee9-4588-9caa-f4b53b6093c0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-760b857b-2ee9-4588-9caa-f4b53b6093c0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   fareamount  ...                                                 id\n","0        4.87  ...  751d10ef2403c770a3bd4e220db8594b656d6774962b63...\n","1       12.70  ...  a9ddc1ab38a3cc3f360e4d2408678d707658762c418e6c...\n","2        5.14  ...  1f804117b3d98193b5ab7fddc15a543a8165cd60b6b20e...\n","3        5.14  ...  21af1912855db837c7892fb073f4c59678c305aec0b23b...\n","4       14.32  ...  26dcdd256e6269e4c6f1ccd2119c345c4deed788a35082...\n","\n","[5 rows x 7 columns]"]},"metadata":{},"execution_count":5}],"source":["df[:5]"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"uY4lLvmL9kne","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643146569125,"user_tz":180,"elapsed":17,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}},"outputId":"00b40d0c-1fbb-4ac6-e916-a0646be9aa95"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 6368133 entries, 0 to 3289205\n","Data columns (total 7 columns):\n"," #   Column                       Dtype  \n","---  ------                       -----  \n"," 0   fareamount                   float64\n"," 1   origindatetime_tr            object \n"," 2   origin_block_latitude        float64\n"," 3   origin_block_longitude       float64\n"," 4   destination_block_latitude   float64\n"," 5   destination_block_longitude  float64\n"," 6   id                           object \n","dtypes: float64(5), object(2)\n","memory usage: 388.7+ MB\n"]}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"D-kVFhml9p0x"},"source":["## Drop the `origindatetime_tr` column from the data frame. \n","\n","For now you are going to predict the taxi fare just based on the lat/lon coordinates of the pickup and the drop off locations. Remove the `origindatetime_tr` column from the data frame in your working dataset."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"jhZpJTVZaas_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643146569559,"user_tz":180,"elapsed":445,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}},"outputId":"b94ff9f4-6de8-4d2d-a2f9-e3835d814e00"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6368133, 6)"]},"metadata":{},"execution_count":7}],"source":["working_df = df.drop('origindatetime_tr', axis = 1)\n","working_df.shape"]},{"cell_type":"markdown","metadata":{"id":"6aA0NkUA_x1M"},"source":["## Sample 10% of your working dataset into a test dataset data frame\n","\n","* **hint:** use the Pandas `sample` function with the dataframe. Specify a value for the `random_state` to achieve reproducibility."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"nsh_vPXiZr9J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643146569834,"user_tz":180,"elapsed":280,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}},"outputId":"9b0f474b-de9e-44ce-95ec-27e3ce547fca"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(636813, 6)"]},"metadata":{},"execution_count":8}],"source":["test_df = working_df.sample(frac = 0.10, random_state = 42)\n","test_df.shape"]},{"cell_type":"markdown","metadata":{"id":"u5FschugACN-"},"source":["## Drop the rows that exist in your test dataset from the working dataset to produce a training dataset.\n","\n","* **hint** DataFrame's `drop` function can use index values from a data frame to drop specific rows."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"CT-b2IlIZ9FP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643146574546,"user_tz":180,"elapsed":4714,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}},"outputId":"d2925fe8-cb42-4e3d-dc3d-4912fb9a99b8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5177451, 6)"]},"metadata":{},"execution_count":9}],"source":["train_df = working_df.drop(index = test_df.index)\n","train_df.shape"]},{"cell_type":"markdown","metadata":{"id":"5R0P1sBeAX15"},"source":["## Define 2 Python lists: 1st for the feature column names; 2nd for the target column name"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"s62k_A-Ga-0x","executionInfo":{"status":"ok","timestamp":1643146574547,"user_tz":180,"elapsed":12,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}}},"outputs":[],"source":["FEATURES = ['origin_block_latitude','origin_block_longitude','destination_block_latitude','destination_block_longitude']\n","TARGET = ['fareamount']"]},{"cell_type":"markdown","metadata":{"id":"ttQDA-m8AgQx"},"source":["## Create `X` and `y` tensors with the values of your feature and target columns in the training dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"hX2dlZgpbA6I","executionInfo":{"status":"ok","timestamp":1643146575140,"user_tz":180,"elapsed":602,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}}},"outputs":[],"source":["X = pt.tensor(train_df[FEATURES].values)\n","y = pt.tensor(train_df[TARGET].values)"]},{"cell_type":"markdown","metadata":{"id":"NQm_SJFDAqqn"},"source":["## Create a `TensorDataset` instance with the `y` and `X` tensors (in that order)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"ffqTuheNbLpj","executionInfo":{"status":"ok","timestamp":1643146575141,"user_tz":180,"elapsed":18,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}}},"outputs":[],"source":["train_ds = TensorDataset(y, X)"]},{"cell_type":"markdown","metadata":{"id":"ElrnaKEtAyEg"},"source":["## Create a `DataLoader` instance specifying a custom batch size\n","\n","A batch size of `2 ** 18 = 262,144` should work well."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"-1YyY4tgbalF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643146575142,"user_tz":180,"elapsed":18,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}},"outputId":"bac22f09-7b25-4a50-945d-da22c69b54da"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["20"]},"metadata":{},"execution_count":13}],"source":["BATCH_SIZE = 2 ** 18\n","train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n","len(train_dl)"]},{"cell_type":"markdown","metadata":{"id":"IA-3bXKABCW_"},"source":["## Create a model using `nn.Linear`"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"6vYGtKDeajQk","executionInfo":{"status":"ok","timestamp":1643146575143,"user_tz":180,"elapsed":12,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}}},"outputs":[],"source":["w = nn.Linear(len(FEATURES), 1)\n"]},{"cell_type":"markdown","metadata":{"id":"u9UvsR9gBGXj"},"source":["## Create an instance of the `AdamW` optimizer for the model"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"vPFF7EtFBKes","executionInfo":{"status":"ok","timestamp":1643146575143,"user_tz":180,"elapsed":11,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}}},"outputs":[],"source":["optimizer = pt.optim.AdamW(w.parameters())"]},{"cell_type":"markdown","metadata":{"id":"tz7LW-TnBNJu"},"source":["## Declare your `forward`, `loss` and `metric` functions\n","\n","* **hint:** if you are tried of computing MSE by hand you can use `nn.functional.mse_loss` instead."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"0T3aWJEZdiVH","executionInfo":{"status":"ok","timestamp":1643146575144,"user_tz":180,"elapsed":11,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}}},"outputs":[],"source":["def forward(X):\n","  return w(X)\n","\n","def loss(y_pred, y):\n","  mse = nn.functional.mse_loss(y_pred, y)\n","  return mse, mse.sqrt()"]},{"cell_type":"markdown","metadata":{"id":"zfu2ejeoBfpQ"},"source":["## Iterate over the batches returned by your `DataLoader` instance\n","\n","For every step of gradient descent, print out the MSE, RMSE, and the batch index\n","* **hint:** you can use Python's `enumerable` for an iterable\n","* **hint:** the batch returned by the `enumerable` has the same contents as your `TensorDataset` instance"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"bVjF8VYwbATZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643146626591,"user_tz":180,"elapsed":51456,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}},"outputId":"d6410d87-7a11-460a-fcaa-54f64d77eb15"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loss:  162.40557078868719  RMSE:  12.743844427357358  Batch Idx:  0\n","Loss:  157.32902296721116  RMSE:  12.543086660276694  Batch Idx:  1\n","Loss:  151.71445317775908  RMSE:  12.317242109245035  Batch Idx:  2\n","Loss:  145.69940587968418  RMSE:  12.070600891409018  Batch Idx:  3\n","Loss:  141.17782961739232  RMSE:  11.881827705256137  Batch Idx:  4\n","Loss:  135.73619647110962  RMSE:  11.650587816548555  Batch Idx:  5\n","Loss:  131.65678235158947  RMSE:  11.47417894019391  Batch Idx:  6\n","Loss:  125.80096598686318  RMSE:  11.216102976830374  Batch Idx:  7\n","Loss:  120.78131545116773  RMSE:  10.990055297912187  Batch Idx:  8\n","Loss:  110.72372013145163  RMSE:  10.52253392161088  Batch Idx:  9\n","Loss:  101.43828374380561  RMSE:  10.071657447699739  Batch Idx:  10\n","Loss:  96.75772865869156  RMSE:  9.836550648407782  Batch Idx:  11\n","Loss:  93.41987616380972  RMSE:  9.665395809991939  Batch Idx:  12\n","Loss:  94.75716708797569  RMSE:  9.734329308584936  Batch Idx:  13\n","Loss:  94.82296206264344  RMSE:  9.737708255161655  Batch Idx:  14\n","Loss:  94.41529462176555  RMSE:  9.71675329633132  Batch Idx:  15\n","Loss:  105.74262354123334  RMSE:  10.283123238648527  Batch Idx:  16\n","Loss:  81.96983972572593  RMSE:  9.053719662421956  Batch Idx:  17\n","Loss:  87.5318241231693  RMSE:  9.355844383227486  Batch Idx:  18\n","Loss:  94.83261747532845  RMSE:  9.738204016928812  Batch Idx:  19\n"]}],"source":["for batch_idx, batch in enumerate(train_dl):\n","  y, X = batch\n","  y_pred = forward(X)\n","  mse, rmse = loss(y_pred, y)\n","  mse.backward()\n","  print(\"Loss: \", mse.item(), \" RMSE: \", rmse.item(), \" Batch Idx: \", batch_idx)\n","  optimizer.step()\n","  optimizer.zero_grad()\n"]},{"cell_type":"markdown","metadata":{"id":"gVl44Jq5CApl"},"source":["## Implement 10 epochs of gradient descent training\n","\n","For every step of gradient descent, printout the MSE, RMSE, epoch index, and batch index.\n","\n","* **hint:** you can call `enumerate(DataLoader)` repeatedly in a `for` loop"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"hHtI3TB8ewaF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643147119600,"user_tz":180,"elapsed":493017,"user":{"displayName":"Axel Sirota","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhF_GvpAITBpH5Xc52kHUBvqdiucPdx4L85mdPGFIs=s64","userId":"02089179879199828401"}},"outputId":"2f9c932d-62c2-4dfe-b125-3e75aff0f213"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Loss:  75.38039074442797  RMSE:  8.682188131135375  Epoch:  0  Batch Idx:  0\n"," Loss:  72.57649468776552  RMSE:  8.519183921466041  Epoch:  0  Batch Idx:  1\n"," Loss:  69.42255570641075  RMSE:  8.332019905545758  Epoch:  0  Batch Idx:  2\n"," Loss:  65.86260107421259  RMSE:  8.115577679636404  Epoch:  0  Batch Idx:  3\n"," Loss:  63.73823582508914  RMSE:  7.983622976136156  Epoch:  0  Batch Idx:  4\n"," Loss:  60.74078185801436  RMSE:  7.793637267541668  Epoch:  0  Batch Idx:  5\n"," Loss:  58.883556055362305  RMSE:  7.673562149051919  Epoch:  0  Batch Idx:  6\n"," Loss:  55.60594412265404  RMSE:  7.456939326738151  Epoch:  0  Batch Idx:  7\n"," Loss:  53.09937395815983  RMSE:  7.2869317245435905  Epoch:  0  Batch Idx:  8\n"," Loss:  47.83739762121358  RMSE:  6.916458459443936  Epoch:  0  Batch Idx:  9\n"," Loss:  42.71465067342297  RMSE:  6.535644625698598  Epoch:  0  Batch Idx:  10\n"," Loss:  40.854123091603796  RMSE:  6.391723014305595  Epoch:  0  Batch Idx:  11\n"," Loss:  39.20203050186332  RMSE:  6.261152489906576  Epoch:  0  Batch Idx:  12\n"," Loss:  40.64483676777243  RMSE:  6.3753303261691805  Epoch:  0  Batch Idx:  13\n"," Loss:  41.862715683979275  RMSE:  6.470140314087422  Epoch:  0  Batch Idx:  14\n"," Loss:  42.585991554886846  RMSE:  6.525794323673314  Epoch:  0  Batch Idx:  15\n"," Loss:  50.95946641941917  RMSE:  7.138589946160178  Epoch:  0  Batch Idx:  16\n"," Loss:  36.20391349434472  RMSE:  6.01696879619171  Epoch:  0  Batch Idx:  17\n"," Loss:  40.8576255719895  RMSE:  6.391996994053541  Epoch:  0  Batch Idx:  18\n"," Loss:  46.093019214153856  RMSE:  6.789183987354729  Epoch:  0  Batch Idx:  19\n"," Loss:  34.28173218696573  RMSE:  5.855060391402102  Epoch:  1  Batch Idx:  0\n"," Loss:  33.37348458884168  RMSE:  5.776978846148017  Epoch:  1  Batch Idx:  1\n"," Loss:  32.21048660618869  RMSE:  5.67542831918338  Epoch:  1  Batch Idx:  2\n"," Loss:  30.579274330578045  RMSE:  5.529853011661164  Epoch:  1  Batch Idx:  3\n"," Loss:  30.2815951980471  RMSE:  5.502871541118064  Epoch:  1  Batch Idx:  4\n"," Loss:  29.093621755873315  RMSE:  5.393850364616478  Epoch:  1  Batch Idx:  5\n"," Loss:  28.822867378245846  RMSE:  5.368693265427431  Epoch:  1  Batch Idx:  6\n"," Loss:  27.354512508288536  RMSE:  5.230154157220276  Epoch:  1  Batch Idx:  7\n"," Loss:  26.55630710669797  RMSE:  5.15328119810068  Epoch:  1  Batch Idx:  8\n"," Loss:  24.64034790485063  RMSE:  4.963904501987385  Epoch:  1  Batch Idx:  9\n"," Loss:  22.308975666139155  RMSE:  4.723237837134517  Epoch:  1  Batch Idx:  10\n"," Loss:  22.19172596650823  RMSE:  4.710809481024278  Epoch:  1  Batch Idx:  11\n"," Loss:  21.421417251396296  RMSE:  4.628327694901939  Epoch:  1  Batch Idx:  12\n"," Loss:  22.606706267563478  RMSE:  4.754651014276808  Epoch:  1  Batch Idx:  13\n"," Loss:  24.29505934333276  RMSE:  4.92900186075566  Epoch:  1  Batch Idx:  14\n"," Loss:  25.461154147600737  RMSE:  5.045904690697273  Epoch:  1  Batch Idx:  15\n"," Loss:  31.51556928599721  RMSE:  5.613872931051897  Epoch:  1  Batch Idx:  16\n"," Loss:  22.463995041710554  RMSE:  4.73961971488331  Epoch:  1  Batch Idx:  17\n"," Loss:  26.166222202875993  RMSE:  5.11529297331795  Epoch:  1  Batch Idx:  18\n"," Loss:  29.736193514847713  RMSE:  5.453090272024452  Epoch:  1  Batch Idx:  19\n"," Loss:  22.494414925006637  RMSE:  4.7428277351182215  Epoch:  2  Batch Idx:  0\n"," Loss:  22.427593007198535  RMSE:  4.735777972751524  Epoch:  2  Batch Idx:  1\n"," Loss:  22.147099958710186  RMSE:  4.706070543320636  Epoch:  2  Batch Idx:  2\n"," Loss:  21.341893462750402  RMSE:  4.619728721770403  Epoch:  2  Batch Idx:  3\n"," Loss:  21.792171803074474  RMSE:  4.668208628914787  Epoch:  2  Batch Idx:  4\n"," Loss:  21.32781399225041  RMSE:  4.618204628667986  Epoch:  2  Batch Idx:  5\n"," Loss:  21.638007952585532  RMSE:  4.651667222898209  Epoch:  2  Batch Idx:  6\n"," Loss:  20.869394587372877  RMSE:  4.568303250373477  Epoch:  2  Batch Idx:  7\n"," Loss:  20.701925721109987  RMSE:  4.549936891992019  Epoch:  2  Batch Idx:  8\n"," Loss:  20.312294328930314  RMSE:  4.506916277115685  Epoch:  2  Batch Idx:  9\n"," Loss:  19.16450232665462  RMSE:  4.377727986827713  Epoch:  2  Batch Idx:  10\n"," Loss:  19.63929600808308  RMSE:  4.431624533744153  Epoch:  2  Batch Idx:  11\n"," Loss:  18.99667849426843  RMSE:  4.358517924050379  Epoch:  2  Batch Idx:  12\n"," Loss:  19.726990734287874  RMSE:  4.441507709583298  Epoch:  2  Batch Idx:  13\n"," Loss:  21.35668010405582  RMSE:  4.621328824489318  Epoch:  2  Batch Idx:  14\n"," Loss:  22.464664237637578  RMSE:  4.739690310309058  Epoch:  2  Batch Idx:  15\n"," Loss:  27.12872376092386  RMSE:  5.208524144220113  Epoch:  2  Batch Idx:  16\n"," Loss:  20.59252391440971  RMSE:  4.537898623196613  Epoch:  2  Batch Idx:  17\n"," Loss:  23.60345048887005  RMSE:  4.858338243563333  Epoch:  2  Batch Idx:  18\n"," Loss:  26.19365475074399  RMSE:  5.117973695784689  Epoch:  2  Batch Idx:  19\n"," Loss:  20.834000903952493  RMSE:  4.564427773987939  Epoch:  3  Batch Idx:  0\n"," Loss:  20.93963743637233  RMSE:  4.575984859718433  Epoch:  3  Batch Idx:  1\n"," Loss:  20.853134905430032  RMSE:  4.566523284231674  Epoch:  3  Batch Idx:  2\n"," Loss:  20.220449645517427  RMSE:  4.496715428567548  Epoch:  3  Batch Idx:  3\n"," Loss:  20.813749823289704  RMSE:  4.562208875456022  Epoch:  3  Batch Idx:  4\n"," Loss:  20.48613354392149  RMSE:  4.526161016128513  Epoch:  3  Batch Idx:  5\n"," Loss:  20.880092482199274  RMSE:  4.569473983096881  Epoch:  3  Batch Idx:  6\n"," Loss:  20.246253900129755  RMSE:  4.499583747429283  Epoch:  3  Batch Idx:  7\n"," Loss:  20.189878894974395  RMSE:  4.493314911618636  Epoch:  3  Batch Idx:  8\n"," Loss:  20.241955093152953  RMSE:  4.499106032663929  Epoch:  3  Batch Idx:  9\n"," Loss:  19.398088936848954  RMSE:  4.4043261614972335  Epoch:  3  Batch Idx:  10\n"," Loss:  19.961495281923384  RMSE:  4.4678289226338315  Epoch:  3  Batch Idx:  11\n"," Loss:  19.251577456490224  RMSE:  4.387661957864373  Epoch:  3  Batch Idx:  12\n"," Loss:  19.7336285264666  RMSE:  4.442254892109029  Epoch:  3  Batch Idx:  13\n"," Loss:  21.26126901798663  RMSE:  4.610994363256871  Epoch:  3  Batch Idx:  14\n"," Loss:  22.27969088024893  RMSE:  4.720136743808269  Epoch:  3  Batch Idx:  15\n"," Loss:  26.473115089720963  RMSE:  5.145203114525311  Epoch:  3  Batch Idx:  16\n"," Loss:  20.61592455922868  RMSE:  4.540476248063487  Epoch:  3  Batch Idx:  17\n"," Loss:  23.38388719707865  RMSE:  4.835688906151702  Epoch:  3  Batch Idx:  18\n"," Loss:  25.678722466271537  RMSE:  5.0674177315740945  Epoch:  3  Batch Idx:  19\n"," Loss:  20.773165863003946  RMSE:  4.557758864069483  Epoch:  4  Batch Idx:  0\n"," Loss:  20.886981347145337  RMSE:  4.570227712832845  Epoch:  4  Batch Idx:  1\n"," Loss:  20.816001859771095  RMSE:  4.562455683047354  Epoch:  4  Batch Idx:  2\n"," Loss:  20.19546975430685  RMSE:  4.493936999370024  Epoch:  4  Batch Idx:  3\n"," Loss:  20.796048567421952  RMSE:  4.5602684753665494  Epoch:  4  Batch Idx:  4\n"," Loss:  20.475846752593853  RMSE:  4.525024502982703  Epoch:  4  Batch Idx:  5\n"," Loss:  20.868557597526852  RMSE:  4.568211641061177  Epoch:  4  Batch Idx:  6\n"," Loss:  20.243927736823082  RMSE:  4.499325253504472  Epoch:  4  Batch Idx:  7\n"," Loss:  20.193098802276438  RMSE:  4.493673197093491  Epoch:  4  Batch Idx:  8\n"," Loss:  20.2980874734564  RMSE:  4.505339884343511  Epoch:  4  Batch Idx:  9\n"," Loss:  19.479664242885487  RMSE:  4.41357726146099  Epoch:  4  Batch Idx:  10\n"," Loss:  20.037089560957757  RMSE:  4.476280773248899  Epoch:  4  Batch Idx:  11\n"," Loss:  19.304852555109235  RMSE:  4.393728775779092  Epoch:  4  Batch Idx:  12\n"," Loss:  19.75401754275191  RMSE:  4.444549194547396  Epoch:  4  Batch Idx:  13\n"," Loss:  21.268286013981065  RMSE:  4.611755198834937  Epoch:  4  Batch Idx:  14\n"," Loss:  22.279401141694628  RMSE:  4.720106051954196  Epoch:  4  Batch Idx:  15\n"," Loss:  26.463641739248402  RMSE:  5.1442824319090805  Epoch:  4  Batch Idx:  16\n"," Loss:  20.61544829710877  RMSE:  4.540423801486902  Epoch:  4  Batch Idx:  17\n"," Loss:  23.387110695291263  RMSE:  4.836022197559815  Epoch:  4  Batch Idx:  18\n"," Loss:  25.70143982074132  RMSE:  5.069658747957432  Epoch:  4  Batch Idx:  19\n"," Loss:  20.772744807828786  RMSE:  4.557712672802969  Epoch:  5  Batch Idx:  0\n"," Loss:  20.886950613313005  RMSE:  4.570224350435436  Epoch:  5  Batch Idx:  1\n"," Loss:  20.81535547790921  RMSE:  4.562384845440946  Epoch:  5  Batch Idx:  2\n"," Loss:  20.194165610347326  RMSE:  4.49379189664445  Epoch:  5  Batch Idx:  3\n"," Loss:  20.794535635218427  RMSE:  4.560102590426934  Epoch:  5  Batch Idx:  4\n"," Loss:  20.47372660509179  RMSE:  4.524790227744464  Epoch:  5  Batch Idx:  5\n"," Loss:  20.867956199711085  RMSE:  4.568145816380108  Epoch:  5  Batch Idx:  6\n"," Loss:  20.241309490887737  RMSE:  4.499034284253426  Epoch:  5  Batch Idx:  7\n"," Loss:  20.188924563138745  RMSE:  4.4932087157329725  Epoch:  5  Batch Idx:  8\n"," Loss:  20.266684259284272  RMSE:  4.5018534249000455  Epoch:  5  Batch Idx:  9\n"," Loss:  19.426669618884837  RMSE:  4.4075695818540215  Epoch:  5  Batch Idx:  10\n"," Loss:  19.977245221347577  RMSE:  4.469591169374172  Epoch:  5  Batch Idx:  11\n"," Loss:  19.251645904698094  RMSE:  4.387669757935082  Epoch:  5  Batch Idx:  12\n"," Loss:  19.72747446031702  RMSE:  4.44156216440984  Epoch:  5  Batch Idx:  13\n"," Loss:  21.256147997474677  RMSE:  4.6104390243744335  Epoch:  5  Batch Idx:  14\n"," Loss:  22.28211218028671  RMSE:  4.720393223057451  Epoch:  5  Batch Idx:  15\n"," Loss:  26.53452450348692  RMSE:  5.15116729523386  Epoch:  5  Batch Idx:  16\n"," Loss:  20.596933219869697  RMSE:  4.538384428391859  Epoch:  5  Batch Idx:  17\n"," Loss:  23.408588261912424  RMSE:  4.838242269865413  Epoch:  5  Batch Idx:  18\n"," Loss:  25.77682023428935  RMSE:  5.077087770985385  Epoch:  5  Batch Idx:  19\n"," Loss:  20.77504820541082  RMSE:  4.557965358074896  Epoch:  6  Batch Idx:  0\n"," Loss:  20.890189068182515  RMSE:  4.570578636035323  Epoch:  6  Batch Idx:  1\n"," Loss:  20.81770531929029  RMSE:  4.56264236153682  Epoch:  6  Batch Idx:  2\n"," Loss:  20.19579478924875  RMSE:  4.493973162942648  Epoch:  6  Batch Idx:  3\n"," Loss:  20.796061372646214  RMSE:  4.560269879365279  Epoch:  6  Batch Idx:  4\n"," Loss:  20.474745875267867  RMSE:  4.524902858102908  Epoch:  6  Batch Idx:  5\n"," Loss:  20.87033535042259  RMSE:  4.568406215566058  Epoch:  6  Batch Idx:  6\n"," Loss:  20.242027404381638  RMSE:  4.499114068834179  Epoch:  6  Batch Idx:  7\n"," Loss:  20.18855919405126  RMSE:  4.493168057623848  Epoch:  6  Batch Idx:  8\n"," Loss:  20.248891929592123  RMSE:  4.4998768793814925  Epoch:  6  Batch Idx:  9\n"," Loss:  19.397508185184634  RMSE:  4.404260231319743  Epoch:  6  Batch Idx:  10\n"," Loss:  19.94643921956503  RMSE:  4.46614366311307  Epoch:  6  Batch Idx:  11\n"," Loss:  19.22627030279596  RMSE:  4.384777109819376  Epoch:  6  Batch Idx:  12\n"," Loss:  19.71649081063176  RMSE:  4.440325529804292  Epoch:  6  Batch Idx:  13\n"," Loss:  21.252350796707454  RMSE:  4.610027201297998  Epoch:  6  Batch Idx:  14\n"," Loss:  22.28495037557763  RMSE:  4.720693844720035  Epoch:  6  Batch Idx:  15\n"," Loss:  26.56537086297977  RMSE:  5.154160539115925  Epoch:  6  Batch Idx:  16\n"," Loss:  20.591112817349543  RMSE:  4.537743141402953  Epoch:  6  Batch Idx:  17\n"," Loss:  23.41777652821106  RMSE:  4.839191722613505  Epoch:  6  Batch Idx:  18\n"," Loss:  25.804137905540593  RMSE:  5.079777348028218  Epoch:  6  Batch Idx:  19\n"," Loss:  20.776751915351824  RMSE:  4.558152247934664  Epoch:  7  Batch Idx:  0\n"," Loss:  20.891964758737522  RMSE:  4.570772884178071  Epoch:  7  Batch Idx:  1\n"," Loss:  20.819022690959923  RMSE:  4.56278672424648  Epoch:  7  Batch Idx:  2\n"," Loss:  20.196753434122478  RMSE:  4.494079820622068  Epoch:  7  Batch Idx:  3\n"," Loss:  20.796848731963685  RMSE:  4.560356206697421  Epoch:  7  Batch Idx:  4\n"," Loss:  20.475300088994956  RMSE:  4.524964098089062  Epoch:  7  Batch Idx:  5\n"," Loss:  20.871018152266906  RMSE:  4.568480945814145  Epoch:  7  Batch Idx:  6\n"," Loss:  20.24233368948946  RMSE:  4.4991481070853245  Epoch:  7  Batch Idx:  7\n"," Loss:  20.188657625096376  RMSE:  4.493179011022861  Epoch:  7  Batch Idx:  8\n"," Loss:  20.247339344933998  RMSE:  4.499704361948016  Epoch:  7  Batch Idx:  9\n"," Loss:  19.39559378956049  RMSE:  4.40404289143061  Epoch:  7  Batch Idx:  10\n"," Loss:  19.94496638664759  RMSE:  4.465978771405837  Epoch:  7  Batch Idx:  11\n"," Loss:  19.225326236304987  RMSE:  4.384669455763454  Epoch:  7  Batch Idx:  12\n"," Loss:  19.7161314615838  RMSE:  4.440285065351525  Epoch:  7  Batch Idx:  13\n"," Loss:  21.252239700583416  RMSE:  4.610015151882195  Epoch:  7  Batch Idx:  14\n"," Loss:  22.285069282500785  RMSE:  4.720706438924241  Epoch:  7  Batch Idx:  15\n"," Loss:  26.566455854514395  RMSE:  5.1542657919935015  Epoch:  7  Batch Idx:  16\n"," Loss:  20.591005359698382  RMSE:  4.537731300958485  Epoch:  7  Batch Idx:  17\n"," Loss:  23.417847664446008  RMSE:  4.839199072619973  Epoch:  7  Batch Idx:  18\n"," Loss:  25.80381724119034  RMSE:  5.0797457850949925  Epoch:  7  Batch Idx:  19\n"," Loss:  20.77664993825978  RMSE:  4.558141061689489  Epoch:  8  Batch Idx:  0\n"," Loss:  20.891767737033426  RMSE:  4.570751331787086  Epoch:  8  Batch Idx:  1\n"," Loss:  20.818793487992636  RMSE:  4.562761607622366  Epoch:  8  Batch Idx:  2\n"," Loss:  20.196514111896672  RMSE:  4.494053194155213  Epoch:  8  Batch Idx:  3\n"," Loss:  20.796582100157465  RMSE:  4.560326972943658  Epoch:  8  Batch Idx:  4\n"," Loss:  20.475049371499686  RMSE:  4.524936394193811  Epoch:  8  Batch Idx:  5\n"," Loss:  20.870617900684735  RMSE:  4.568437139841669  Epoch:  8  Batch Idx:  6\n"," Loss:  20.242091807883522  RMSE:  4.4991212261822335  Epoch:  8  Batch Idx:  7\n"," Loss:  20.18855330659267  RMSE:  4.493167402467069  Epoch:  8  Batch Idx:  8\n"," Loss:  20.249783012068246  RMSE:  4.499975890165218  Epoch:  8  Batch Idx:  9\n"," Loss:  19.400042252725466  RMSE:  4.404547905600014  Epoch:  8  Batch Idx:  10\n"," Loss:  19.949799422411054  RMSE:  4.466519833428601  Epoch:  8  Batch Idx:  11\n"," Loss:  19.229150079847123  RMSE:  4.385105481040008  Epoch:  8  Batch Idx:  12\n"," Loss:  19.717588077332397  RMSE:  4.440449085096281  Epoch:  8  Batch Idx:  13\n"," Loss:  21.252636979690234  RMSE:  4.610058240379424  Epoch:  8  Batch Idx:  14\n"," Loss:  22.284731419613742  RMSE:  4.720670653584482  Epoch:  8  Batch Idx:  15\n"," Loss:  26.563838851905253  RMSE:  5.154011918098876  Epoch:  8  Batch Idx:  16\n"," Loss:  20.591442976321627  RMSE:  4.537779520461701  Epoch:  8  Batch Idx:  17\n"," Loss:  23.41714713893923  RMSE:  4.8391266917636315  Epoch:  8  Batch Idx:  18\n"," Loss:  25.801808014651314  RMSE:  5.079548012830601  Epoch:  8  Batch Idx:  19\n"," Loss:  20.776474236764606  RMSE:  4.5581217882769005  Epoch:  9  Batch Idx:  0\n"," Loss:  20.89154467670677  RMSE:  4.570726930883835  Epoch:  9  Batch Idx:  1\n"," Loss:  20.818588015355513  RMSE:  4.562739091308587  Epoch:  9  Batch Idx:  2\n"," Loss:  20.19632994485665  RMSE:  4.494032704026157  Epoch:  9  Batch Idx:  3\n"," Loss:  20.796397150247174  RMSE:  4.560306694757182  Epoch:  9  Batch Idx:  4\n"," Loss:  20.47488992937124  RMSE:  4.524918775997116  Epoch:  9  Batch Idx:  5\n"," Loss:  20.870371258849058  RMSE:  4.568410145646848  Epoch:  9  Batch Idx:  6\n"," Loss:  20.2419558708961  RMSE:  4.499106119097004  Epoch:  9  Batch Idx:  7\n"," Loss:  20.18850720505938  RMSE:  4.493162272282115  Epoch:  9  Batch Idx:  8\n"," Loss:  20.251254842778167  RMSE:  4.500139424815432  Epoch:  9  Batch Idx:  9\n"," Loss:  19.402610359595904  RMSE:  4.404839424950233  Epoch:  9  Batch Idx:  10\n"," Loss:  19.952406935656192  RMSE:  4.4668117192978025  Epoch:  9  Batch Idx:  11\n"," Loss:  19.230980498015995  RMSE:  4.3853141846412775  Epoch:  9  Batch Idx:  12\n"," Loss:  19.718149301062507  RMSE:  4.440512279125294  Epoch:  9  Batch Idx:  13\n"," Loss:  21.252736987453503  RMSE:  4.61006908705862  Epoch:  9  Batch Idx:  14\n"," Loss:  22.28471434642777  RMSE:  4.72066884524087  Epoch:  9  Batch Idx:  15\n"," Loss:  26.56439224299833  RMSE:  5.154065603288178  Epoch:  9  Batch Idx:  16\n"," Loss:  20.591290238174427  RMSE:  4.537762690817406  Epoch:  9  Batch Idx:  17\n"," Loss:  23.417540599950776  RMSE:  4.839167345727029  Epoch:  9  Batch Idx:  18\n"," Loss:  25.803190701800034  RMSE:  5.079684114371683  Epoch:  9  Batch Idx:  19\n"]}],"source":["for epoch in range(10):\n","  for batch_idx, batch in enumerate(train_dl):\n","    y, X = batch\n","    y_pred = forward(X)\n","    mse, rmse = loss(y_pred, y)\n","    mse.backward()\n","    print(\" Loss: \", mse.item(), \" RMSE: \", rmse.item(), \" Epoch: \", epoch, \" Batch Idx: \", batch_idx)\n","    optimizer.step()  \n","    optimizer.zero_grad()"]},{"cell_type":"markdown","metadata":{"id":"IlrLCc3SCmuk"},"source":["Copyright 2021 CounterFactual.AI LLC. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Solution_PyTorch_Dataset_Dataloader.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":0}