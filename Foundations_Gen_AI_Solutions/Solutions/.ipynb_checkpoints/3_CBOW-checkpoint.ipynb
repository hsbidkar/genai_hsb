{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jbFcxFZhG5K"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade  textblob gensim pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iklSJ4lqUQlT"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "import sys\n",
    "from textblob import TextBlob, Word\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "\n",
    "TRACE = False  # Setting to true is useful when debugging to know which device is being used\n",
    "embedding_dim = 50\n",
    "epochs=100\n",
    "batch_size = 100\n",
    "BATCH = True\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  random.seed(42)\n",
    "\n",
    "\n",
    "set_seeds_and_trace()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "textblob_tokenizer = lambda x: TextBlob(x).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l13de14sclyD"
   },
   "outputs": [],
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f yelp.csv ]; then\n",
    "  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvRXU9EMVJMp"
   },
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAWXcLEieD4E"
   },
   "outputs": [],
   "source": [
    "path = './yelp.csv'\n",
    "yelp = pd.read_csv(path)\n",
    "# Create a new DataFrame that only contains the 5-star and 1-star reviews.\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars.map({1:0, 5:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljgSnKkzeM4-"
   },
   "outputs": [],
   "source": [
    "yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUlTe1xsgi51"
   },
   "outputs": [],
   "source": [
    "TextBlob(X.values[0]).correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OgH8jUM7XvU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfR6qIZZhIHd"
   },
   "outputs": [],
   "source": [
    "# Create corpus of sentences such that the sentence has more than 3 words\n",
    "corpus = [line for line in X.values if len(TextBlob(line).words)> 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2B_z5Udki-_s"
   },
   "outputs": [],
   "source": [
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_Z1eJZrhK7K",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At this point we have a list (any iterable will do) of queries that are longer than 3 words. This is normal to filter random queries. Now we must use the `LabelEncoder` object to `fit` on the corpus, in order to convert each wor to an ID, and later convert such corpus of list of words into their identifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHtu75Kpi6XF"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "\n",
    "\n",
    "list_of_words = list(itertools.chain.from_iterable([sentence.split() for sentence in corpus]))\n",
    "ids_from_words = LabelEncoder(list_of_words, reserved_labels=['UNK'], unknown_index=0, min_occurrences=1)\n",
    "\n",
    "\n",
    "# tokenizer = Tokenizer()\n",
    "# # Use the fit_on_texts method to fit the tokenizer\n",
    "# tokenizer.fit_on_texts(corpus) # Fill\n",
    "\n",
    "print(f'Before the tokenizer: {corpus[:1]}')\n",
    "\n",
    "#Now use the same \"trained\" tokenizer to convert the corpus from words to IDs with the texts_to_sequences method\n",
    "tokenized_corpus = [ids_from_words.batch_encode(sentence.split()) for sentence in corpus]\n",
    "\n",
    "print(f'After the tokenizer: {tokenized_corpus[:1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g44ICdUcj7ZL"
   },
   "outputs": [],
   "source": [
    "vocab_size = LabelEncoder.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTM2wqbzke5n"
   },
   "outputs": [],
   "source": [
    "print(f'First 5 corpus items are {tokenized_corpus[:5]}')\n",
    "print(f'Length of corpus is {len(tokenized_corpus)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GR97HVOqkoMI"
   },
   "outputs": [],
   "source": [
    "type(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6B_A4TL9awJ"
   },
   "outputs": [],
   "source": [
    "def ids_from_text(text):\n",
    "  return ids_from_words.batch_encode(text)\n",
    "\n",
    "def text_from_ids(ids):\n",
    "  return ids_from_words.batch_decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEtqKkd9_ZmZ"
   },
   "outputs": [],
   "source": [
    "def pad_sequence_of_tokens(x, maxlen, unk_token='UNK'):\n",
    "  if len(x)<maxlen:\n",
    "    x.extend([unk_token]*(maxlen-len(x)))\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvMp9eWsk2Z-"
   },
   "outputs": [],
   "source": [
    "# This is the algorithmic part of batching the dataset and yielding the window of words and expected middle word for each bacth as a generator.\n",
    "def create_context_target_pairs(texts, context_size):\n",
    "    data = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        for i, word in enumerate(tokens):\n",
    "            start = max(0, i - context_size)\n",
    "            end = min(len(tokens), i + context_size + 1)\n",
    "            context = pad_sequence_of_tokens([tokens[j] for j in range(start, end) if j != i], maxlen=4)\n",
    "            target = ids_from_words.token_to_index[word]\n",
    "            context_indices = [ids_from_words.token_to_index[w] for w in context]\n",
    "            context_indices.append(target)\n",
    "            data.append(torch.Tensor(context_indices))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0dw_S7Kk6lW",
    "outputId": "ce29f09a-b1b5-4cc1-d432-3963bde9cf3d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice now in a sample how we construct X and y to predict words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCmSyCj8k6He"
   },
   "outputs": [],
   "source": [
    "data = create_context_target_pairs(corpus[:500], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WsntEC8p-2Xj"
   },
   "outputs": [],
   "source": [
    "data = torch.stack(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOoN-ej4AMQI"
   },
   "outputs": [],
   "source": [
    "X = data[:, :4].to(torch.long)\n",
    "y = data[:, 4].to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hfz_UgdkAVVx"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjvrCdY5lkJk"
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cIBBOa4a6qM"
   },
   "source": [
    "Now comes the core part, defining the model. Keras provides a convenient Sequential model class to just `add` layers of any type and they will just work. Let's add an `Embedding` layer (that will map the word ids into a vector of size 100), a `Lambda` to average the words out in a sentence, and a `Dense layer` to select the best word on the other end. This is classic CBOW.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1701060609289,
     "user": {
      "displayName": "Axel Sirota",
      "userId": "02089179879199828401"
     },
     "user_tz": 180
    },
    "id": "NjtywT2Fa6qM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        print((vocab_size, embedding_dim))\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Linear layer to act as the hidden layer\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        # Linear layer to predict the center word\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds = torch.mean(embeds, dim=1)\n",
    "        out = torch.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = torch.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBNNSCeYa6qM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_cbow(X, y, model, loss_function, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        # Step 1. Recall that torch *accumulates* gradients. Before passing in a new instance,\n",
    "        # you need to zero out the gradients from the old instance\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2. Run the forward pass, getting log probabilities over next words\n",
    "        log_probs = model(X)\n",
    "\n",
    "        # Step 3. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, y)\n",
    "\n",
    "        # Step 4. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print('Epoch: {}, Loss: {:.4f}'.format(epoch + 1, total_loss))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpEmERLpa6qM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "context_size=2\n",
    "embedding_dim=50\n",
    "vocab_size = len(ids_from_words.vocab)\n",
    "model = CBOW(vocab_size, embedding_dim, context_size * 2)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSi6tkuJA2l2"
   },
   "outputs": [],
   "source": [
    "trained_model = train_cbow(X, y, model, loss_function, optimizer, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QacFd_6Da6qN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "embeddings = trained_model.embeddings.weight.data.cpu().numpy()\n",
    "\n",
    "# Now, we need to save these embeddings in a format that gensim can understand\n",
    "# For that, we will use the KeyedVectors instance in gensim\n",
    "\n",
    "# Instantiate the KeyedVectors with the correct size\n",
    "kv = KeyedVectors(vector_size=embeddings.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1fC32QFHIWF"
   },
   "outputs": [],
   "source": [
    "# Add the vectors and their corresponding words to the KeyedVectors instance\n",
    "kv.add_vectors(ids_from_words.index_to_token, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsEzF8O1a6qN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kv.most_similar(positive=['gasoline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDAAzHMka6qN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kv.most_similar(negative=['apple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_wQsdlCa6qO",
    "pycharm": {
     "name": "#%%"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uCMkd6qbRxk"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
