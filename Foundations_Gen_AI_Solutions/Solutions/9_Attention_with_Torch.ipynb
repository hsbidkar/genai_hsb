{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention with Frameworks\n",
        "\n",
        "© Data Trainers LLC. GPL v 3.0.\n",
        "\n",
        "Author: Axel Sirota"
      ],
      "metadata": {
        "id": "r45tbR6lkYEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A whole new world opportunities appear when considering using the layer implementations of the attention components. As of July 2023 we have 1 layers implemented:\n",
        "\n",
        "- MultiHeadAttention: The general attention everyone uses and we will learn in this demo! It is basically many layers of self attention.\n",
        "\n",
        "Let's get to it!\n"
      ],
      "metadata": {
        "id": "hcU-PBR3kbaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep"
      ],
      "metadata": {
        "id": "JsTW2QHimUlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade  textblob gensim pytorch-nlp swifter\n"
      ],
      "metadata": {
        "id": "C9BTEOu0PerV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "842c5c42-0ab6-4ee2-f2b9-019b177b8d03"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Collecting pytorch-nlp\n",
            "  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting swifter\n",
            "  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-nlp) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from swifter) (1.5.3)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.10/dist-packages (from swifter) (5.9.5)\n",
            "Requirement already satisfied: dask[dataframe]>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from swifter) (2023.8.1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (23.2)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (1.4.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (0.12.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (6.8.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter) (2023.3.post1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]>=2.10.0->swifter) (3.17.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.0->swifter) (1.16.0)\n",
            "Building wheels for collected packages: swifter\n",
            "  Building wheel for swifter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16505 sha256=bc0561d65b01c714fd8d515357510278919e09b743da4c6976356f1c4983a2c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/cf/51/0904952972ee2c7aa3709437065278dc534ec1b8d2ad41b443\n",
            "Successfully built swifter\n",
            "Installing collected packages: pytorch-nlp, swifter\n",
            "Successfully installed pytorch-nlp-0.5.0 swifter-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run some helper functions to setup using the GPUs"
      ],
      "metadata": {
        "id": "9H2IYs8QkXZd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0fpgYwAtNO2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99968411-df73-40f8-af32-68c218715578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import sys\n",
        "from textblob import TextBlob, Word\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import swifter\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import warnings\n",
        "import nltk\n",
        "\n",
        "max_length = 100\n",
        "# Hyperparameters\n",
        "embedding_dim = 100  # embedding dimension\n",
        "hidden_dim = 100  # LSTM hidden dimensions\n",
        "num_layers = 1  # number of LSTM layers\n",
        "batch_size = 64  # batch size\n",
        "\n",
        "\n",
        "def set_seeds_and_trace():\n",
        "  os.environ['PYTHONHASHSEED'] = '0'\n",
        "  np.random.seed(42)\n",
        "  random.seed(42)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "set_seeds_and_trace()\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt')\n",
        "textblob_tokenizer = lambda x: TextBlob(x).words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Attention"
      ],
      "metadata": {
        "id": "uvVC_dlqmdf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The easiest way to test attention in PyTorch is to create a simple model that uses such a layer, we will do just that! This also shows how easy is to add attention to your models, which we will use extensively when creating THE Transformer from scratch"
      ],
      "metadata": {
        "id": "ibxTc61LmmhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice we need a custom model class because the inputs needs to be the query and value, and they could have different embeddings as well."
      ],
      "metadata": {
        "id": "oLbdmNwEoUM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        \"\"\"\n",
        "        Forward pass for the Dot Product Attention.\n",
        "\n",
        "        Args:\n",
        "        - query: A tensor of shape (batch_size, query_length, dimensions)\n",
        "        - key: A tensor of shape (batch_size, key_length, dimensions)\n",
        "        - value: A tensor of shape (batch_size, value_length, dimensions) where key_length == value_length\n",
        "\n",
        "        Returns:\n",
        "        - The context vector and the attention weights.\n",
        "        \"\"\"\n",
        "        # Calculate the scores\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / (key.size(-1) ** 0.5)\n",
        "        # Apply the softmax to get attention weights\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        # Create the context vector\n",
        "        context = torch.matmul(attention_weights, value)\n",
        "        return context, attention_weights"
      ],
      "metadata": {
        "id": "36P0ESmxThvQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "query_length = 20\n",
        "key_value_length = 10\n",
        "dimensions = 3"
      ],
      "metadata": {
        "id": "WpLSon3cNVvc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DotProductAttention()"
      ],
      "metadata": {
        "id": "CSRhli0ZWFxf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oh no! We need to call the model, well that is simple let's simulate 3 sentences!"
      ],
      "metadata": {
        "id": "eXOb_IEGqV2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = torch.randn(batch_size, query_length, dimensions)\n",
        "key = torch.randn(batch_size, key_value_length, dimensions)\n",
        "value = torch.randn(batch_size, key_value_length, dimensions)"
      ],
      "metadata": {
        "id": "Mtn3d4wwyEGU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context, attention_weights = model(query, key, value)"
      ],
      "metadata": {
        "id": "8ngnlesuSTyC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context.shape"
      ],
      "metadata": {
        "id": "bZCFervbyf7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "748e0960-1792-4503-dc60-0aa86fecb229"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 20, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7XwdtA14Gjw",
        "outputId": "c03c6a3c-6d0c-42c6-bb3e-dd5c8798931d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 20, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that attention adds very few parameters, adds many knowledge to the following layers, and is paralellizable."
      ],
      "metadata": {
        "id": "nWQHcNquqfVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiHead Attention"
      ],
      "metadata": {
        "id": "A5y1okQ9QJ48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you are ready to see Multi Head Attention. The idea is quite simple, as in CNNs we had many filters and each convolution checked many different aspects of an image, having many self attentions can check different aspects of our entity, globally. In image it is:\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://www.dropbox.com/s/wjfxpap06viclhv/mha.png?raw=1'  />\n",
        "<figcaption>Attention</figcaption></center>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "bakdsYuhqogu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each head performs Scaled attention as we did before with the weird formula, and then we concatenate!"
      ],
      "metadata": {
        "id": "S4tqYQY3rPCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 4  # Embedding size\n",
        "num_heads = 2   # Number of attention heads\n",
        "sequence_length = 3  # Sequence length for each input\n",
        "batch_size = 1  # Batch size"
      ],
      "metadata": {
        "id": "3zdglUVG5n2I"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionModel(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super(MultiHeadAttentionModel, self).__init__()\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embed_size, num_heads=num_heads)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        # In practice, attention is often applied to a sequence of embeddings with padding.\n",
        "        # Attention mask could be used to ignore the padding or past/future tokens.\n",
        "        # Here we do not use such masks for simplicity.\n",
        "\n",
        "        # MultiheadAttention requires the input of shape (sequence_length, batch_size, embed_size)\n",
        "        query = query.transpose(0, 1)  # Transpose for the multihead attention input requirements\n",
        "        key = key.transpose(0, 1)\n",
        "        value = value.transpose(0, 1)\n",
        "\n",
        "        # Forward pass of the multihead attention\n",
        "        # attn_output is the attention applied embeddings (context vectors)\n",
        "        # attn_output_weights are the attention weights\n",
        "        attn_output, attn_output_weights = self.multihead_attn(query, key, value)\n",
        "        return attn_output, attn_output_weights\n"
      ],
      "metadata": {
        "id": "7TqMEi2gQNFZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "multihead_attn_model = MultiHeadAttentionModel(embed_size, num_heads)\n",
        "\n",
        "# Dummy data with sequence first format\n",
        "query = torch.randn(sequence_length, batch_size, embed_size)\n",
        "key = torch.randn(sequence_length, batch_size, embed_size)\n",
        "value = torch.randn(sequence_length, batch_size, embed_size)\n"
      ],
      "metadata": {
        "id": "HyWkokOtw6GW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass\n",
        "attn_output, attn_output_weights = multihead_attn_model(query, key, value)\n"
      ],
      "metadata": {
        "id": "vXhcalSwxF9a"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose back to (batch_size, sequence_length, embed_size) for the output\n",
        "attn_output = attn_output.transpose(0, 1)\n",
        "\n",
        "attn_output, attn_output_weights"
      ],
      "metadata": {
        "id": "HW4raKPVxPNj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffbeedf2-f9e2-46a1-9834-b740ccc82462"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.3791,  0.1048,  0.2742, -0.0339]],\n",
              " \n",
              "         [[ 1.2148,  0.9171,  0.1215, -1.4806]],\n",
              " \n",
              "         [[-0.8146, -0.5616, -0.2125,  0.4550]]], grad_fn=<TransposeBackward0>),\n",
              " tensor([[[1.]],\n",
              " \n",
              "         [[1.]],\n",
              " \n",
              "         [[1.]]], grad_fn=<MeanBackward1>))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output.shape"
      ],
      "metadata": {
        "id": "HelBLsdnxbL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b54c02a4-ac61-4897-8d09-e593e3cb2cce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 1, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output_weights.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqVlDZGC5zja",
        "outputId": "fc1a4b96-ae61-4258-ae7b-184079525759"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FbR44Su86N9h"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Can you guess each value in the response.shape where does it come from?**"
      ],
      "metadata": {
        "id": "NrYWnu1RsQWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, notice Attention as complex as multi head attention did not add many params and adds a lot lexical intelligence."
      ],
      "metadata": {
        "id": "n-KQ5uhrsfmB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C3oKCxnNXCot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}