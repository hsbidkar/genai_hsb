{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGC5zBa03SRC"
   },
   "source": [
    "# Using GloVe Embedding\n",
    "\n",
    "Â© Data Trainers LLC. GPL v 3.0.\n",
    "\n",
    "**Author:** Axel Sirota\n",
    "\n",
    "\n",
    "In this notebook we will leverage Standford's GloVe vectors which is a pretrained embedding on 1.4B Tweets.\n",
    "\n",
    "Take it easy and pay attention to the main differences with before, and the non-trainable parameters. Finally, check how accurate the results now are.\n",
    "You can run this lab both locally or in Colab.\n",
    "\n",
    "- To run in Colab just go to `https://colab.research.google.com`, sign-in and you upload this notebook. Colab has GPU access for free.\n",
    "- To run locally just run `jupyter notebook` and access the notebook in this lab. You would need to first install the requirements in `requirements.txt`\n",
    "\n",
    "Follow the instructions. Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75x9mZNIbPAp"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUsYzWxfT8o-"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade  textblob gensim pytorch-nlp swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3XUwgb0UBut"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "import sys\n",
    "from textblob import TextBlob, Word\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import nltk\n",
    "\n",
    "TRACE = False  # Setting to true is useful when debugging to know which device is being used\n",
    "embedding_dim = 100\n",
    "epochs=10\n",
    "\n",
    "def set_seeds_and_trace():\n",
    "  os.environ['PYTHONHASHSEED'] = '0'\n",
    "  np.random.seed(42)\n",
    "  random.seed(42)\n",
    "\n",
    "\n",
    "set_seeds_and_trace()\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "textblob_tokenizer = lambda x: TextBlob(x).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tx3hZd6gUImG"
   },
   "outputs": [],
   "source": [
    "%%writefile get_data.sh\n",
    "if [ ! -f yelp.csv ]; then\n",
    "  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
    "fi\n",
    "\n",
    "if [ ! -f glove.6B.100d.txt ]; then\n",
    "  wget -O glove.6B.100d.txt https://www.dropbox.com/s/dl1vswq2sz5f1ws/glove.6B.100d.txt?dl=0\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfyMUL8nXRYP"
   },
   "outputs": [],
   "source": [
    "!bash get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ckCWLwTXVa0"
   },
   "outputs": [],
   "source": [
    "path = './yelp.csv'\n",
    "yelp = pd.read_csv(path)\n",
    "# Create a new DataFrame that only contains the 5-star and 1-star reviews to have extremes.\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars.map({1:0, 5:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yV8JS-8harja"
   },
   "outputs": [],
   "source": [
    "corpus = [sentence for sentence in X.values if type(sentence) == str and len(TextBlob(sentence).words) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B__Of4dUXXBU"
   },
   "outputs": [],
   "source": [
    "path_to_glove_file = \"./glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "# Construct a function that fills the embedding_index dict for every word in the GloVe file with its coefficients.\n",
    "# HELP: For that iterate over the Glove file (hint: check that file to view its structure first!), split the word from the numbers, and populate the dictionary with the word and the numbers as a numpy array.\n",
    "# Hint2: check np.fromstring\n",
    "# FILL\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fygJjt56Xhup"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "\n",
    "\n",
    "list_of_words = list(itertools.chain.from_iterable([sentence.split() for sentence in corpus]))\n",
    "ids_from_words = LabelEncoder(list_of_words, reserved_labels=['UNK'], unknown_index=0, min_occurrences=1)\n",
    "\n",
    "\n",
    "# tokenizer = Tokenizer()\n",
    "# # Use the fit_on_texts method to fit the tokenizer\n",
    "# tokenizer.fit_on_texts(corpus) # Fill\n",
    "\n",
    "print(f'Before the tokenizer: {corpus[:1]}')\n",
    "\n",
    "#Now use the same \"trained\" tokenizer to convert the corpus from words to IDs with the texts_to_sequences method\n",
    "tokenized_corpus = [ids_from_words.batch_encode(sentence.split()) for sentence in corpus]\n",
    "\n",
    "print(f'After the tokenizer: {tokenized_corpus[:1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def ids_from_text(text):\n",
    "  return ids_from_words.batch_encode(text)\n",
    "\n",
    "def text_from_ids(ids):\n",
    "  return ids_from_words.batch_decode(ids)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bp6wXMtvXoJ4"
   },
   "outputs": [],
   "source": [
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Create a loop such that for every word in the vocabulary, if it exists in the Glove embedding, then set for that word (that means that index) the tensor of the Glove Embedding. Otherwise fill with 0\n",
    "# FILL\n",
    "\n",
    "# In the end, the embedding matrix should have, for every word of our tokenizer that exists in GloVe, the tensor representation of that word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pad_sequence_of_tokens(x, maxlen, unk_token='UNK'):\n",
    "  if len(x)<maxlen:\n",
    "    x.extend([unk_token]*(maxlen-len(x)))\n",
    "  return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This is the algorithmic part of batching the dataset and yielding the window of words and expected middle word for each bacth as a generator.\n",
    "def create_context_target_pairs(texts, context_size):\n",
    "    data = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        for i, word in enumerate(tokens):\n",
    "            start = max(0, i - context_size)\n",
    "            end = min(len(tokens), i + context_size + 1)\n",
    "            context = pad_sequence_of_tokens([tokens[j] for j in range(start, end) if j != i], maxlen=4)\n",
    "            target = ids_from_words.token_to_index[word]\n",
    "            context_indices = [ids_from_words.token_to_index[w] for w in context]\n",
    "            context_indices.append(target)\n",
    "            data.append(torch.Tensor(context_indices))\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = create_context_target_pairs(corpus[:500], 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = torch.stack(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = data[:, :4].to(torch.long)\n",
    "y = data[:, 4].to(torch.long)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = None # Instantiate the Embedding with the matrix we created\n",
    "        # Linear layer to act as the hidden layer\n",
    "        self.linear1 = None # Use the linear layer as before\n",
    "        # Linear layer to predict the center word\n",
    "        self.linear2 = None # Use the linear layer as before\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass  # Implement"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_cbow(X, y, model, loss_function, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        # Step 1. Recall that torch *accumulates* gradients. Before passing in a new instance,\n",
    "        # you need to zero out the gradients from the old instance\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2. Run the forward pass, getting log probabilities over next words\n",
    "        log_probs = model(X)\n",
    "\n",
    "        # Step 3. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, y)\n",
    "\n",
    "        # Step 4. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print('Epoch: {}, Loss: {:.4f}'.format(epoch + 1, total_loss))\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "context_size=2\n",
    "embedding_dim=100\n",
    "vocab_size = len(ids_from_words.vocab)\n",
    "model = CBOW(vocab_size, embedding_dim, context_size * 2)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trained_model = train_cbow(X, y, model, loss_function, optimizer, epochs=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "embeddings = trained_model.embeddings.weight.data.cpu().numpy()\n",
    "\n",
    "# Now, we need to save these embeddings in a format that gensim can understand\n",
    "# For that, we will use the KeyedVectors instance in gensim\n",
    "\n",
    "# Instantiate the KeyedVectors with the correct size\n",
    "kv = KeyedVectors(vector_size=embeddings.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add the vectors and their corresponding words to the KeyedVectors instance\n",
    "kv.add_vectors(ids_from_words.index_to_token, embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kv.most_similar(positive=['gasoline'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kv.most_similar(negative=['apple'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "gpuClass": "premium"
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}