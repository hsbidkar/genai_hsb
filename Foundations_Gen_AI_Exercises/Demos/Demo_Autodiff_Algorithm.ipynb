{"cells":[{"cell_type":"markdown","metadata":{"id":"X4QWrv6rKQ04"},"source":["## Import the __`torch`__ package"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vw2yu0HIKQ05"},"outputs":[],"source":["import torch as pt\n","pt.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ghW9x_7LKQ09"},"outputs":[],"source":["class Scalar:\n","    def __init__(self, val):\n","        self.val = val\n","        self.grad = 0.\n","        self.backward = lambda: None\n","\n","    def __repr__(self):\n","        return f\"Value: {self.val}, Gradient: {self.grad}\"\n","\n","    def __add__(self, other):\n","        result = Scalar(self.val + other.val)\n","        return result\n","\n","    def __mul__(self, other):\n","        result = Scalar(self.val * other.val)\n","        return result"]},{"cell_type":"markdown","metadata":{"id":"GqeyPMtaKQ1A"},"source":["## Create a `Scalar` instance for `x = 2.0`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UdZXolFUKQ1B"},"outputs":[],"source":["x = Scalar(2.)\n","x"]},{"cell_type":"markdown","metadata":{"id":"wpCVxuQcKQ1E"},"source":["## Define `y = x`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bu8de4X4KQ1E"},"outputs":[],"source":["y = x"]},{"cell_type":"markdown","metadata":{"id":"9Qq9ScpmKQ1H"},"source":["## Prepare for and call `backward` on `y`\n","* Use floating point values\n","* Zero out the accumulating gradients\n","* Initialize $ \\frac{\\partial y}{ \\partial y} $\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WAHVfnC_KQ1I"},"outputs":[],"source":["x.grad = 0.\n","y.grad = 1.\n","y.backward()"]},{"cell_type":"markdown","metadata":{"id":"NRbBgulCKQ1L"},"source":["* check that $ \\frac{\\partial y}{ \\partial x} = 1.0 $"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBimRd4cKQ1L"},"outputs":[],"source":["y.grad"]},{"cell_type":"markdown","metadata":{"id":"gC5POt4UKQ1P"},"source":["## Self-check\n","\n","* Why the did the implementation return the correct answer?"]},{"cell_type":"markdown","metadata":{"id":"e-JVg1nkKQ1P"},"source":["## Implement `backward` support in the` __add__` function\n","* **hint:** given $ y = a + b $, you need to update `a.grad` and `b.grad` to accumulate $ \\frac{\\partial y}{ \\partial a} $ and $ \\frac{\\partial y}{ \\partial b} $ respectively.\n","\n","\n","\n","* **hint:** don't forget to make the recursive `backward` call"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALT7TH5LKQ1Q"},"outputs":[],"source":["class Scalar:\n","    def __init__(self, val):\n","        self.val = val\n","        self.grad = 0.\n","        self.backward = lambda: None\n","\n","    def __repr__(self):\n","        return f\"Value: {self.val}, Gradient: {self.grad}\"\n","\n","    def __add__(self, other):\n","        result = Scalar(self.val + other.val)\n","        def backward():\n","            print(f'Other.grad: {other.grad}, self.grad = {self.grad}, result.grad = {result.grad}')\n","            self.grad += result.grad\n","            other.grad += result.grad\n","            self.backward(), other.backward()\n","        result.backward = backward\n","        return result\n","\n","    def __mul__(self, other):\n","        result = Scalar(self.val * other.val)\n","        return result\n"]},{"cell_type":"markdown","metadata":{"id":"xoQ8FV8DKQ1S"},"source":["## Define `y = 3 * x` for `x = 3.0`\n","* **hint:** recall that $ 3 * x = x + x + x $"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s88Zk_6cKQ1T"},"outputs":[],"source":["x = Scalar(3.0)\n","y = x + x + x"]},{"cell_type":"markdown","metadata":{"id":"vx3zDy3dKQ1V"},"source":["## Prepare for and run the backward pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKtdBqCIKQ1W"},"outputs":[],"source":["x.grad = 0.\n","y.grad = 1.\n","y.backward()"]},{"cell_type":"markdown","metadata":{"id":"xw8otmYLKQ1Z"},"source":["* check that $ \\frac{\\partial y}{ \\partial x} = 3.0 $"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PaZsCpANKQ1Z"},"outputs":[],"source":["x.grad"]},{"cell_type":"markdown","metadata":{"id":"FA2ggcfHKQ1c"},"source":["## Implement `backward` support in the` __mul__` function\n","* **hint:** given $ y = c * x$, $ \\frac{\\partial y}{ \\partial x} = c $"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PuR5VHEBKQ1d"},"outputs":[],"source":["class Scalar:\n","    def __init__(self, val):\n","        self.val = val\n","        self.grad = 0.\n","        self.backward = lambda: None\n","\n","    def __repr__(self):\n","        return f\"Value: {self.val}, Gradient: {self.grad}\"\n","\n","    def __add__(self, other):\n","        result = Scalar(self.val + other.val)\n","        def backward():\n","            self.grad += result.grad\n","            other.grad += result.grad\n","            self.backward()\n","            other.backward()\n","        result.backward = backward\n","        return result\n","\n","    def __mul__(self, other):\n","        result = Scalar(self.val * other.val)\n","        def backward():\n","            self.grad += other.val * result.grad\n","            other.grad += self.val * result.grad\n","            self.backward()\n","            other.backward()\n","        result.backward = backward\n","        return result"]},{"cell_type":"markdown","metadata":{"id":"ReYBUarZKQ1f"},"source":["## Use `y = x^3 + 2*x` for `x = 4.0`\n","* **hint:** recall that $ x^3 = x * x * x $"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0VnU35HOKQ1g"},"outputs":[],"source":["x = Scalar(4.0)\n","y = x * x * x + x + x"]},{"cell_type":"markdown","metadata":{"id":"fr15Uw_cKQ1i"},"source":["* given $ y = x^3 + 2x $ the analytical solution to $ \\frac{\\partial y}{ \\partial x} = 3x^2+2 $\n","* check that your implementation of `Scalar` returns the correct value of $ \\frac{\\partial y}{ \\partial x} $ when $ x = 4.0 $"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"um4aAcr9KQ1i"},"outputs":[],"source":["x.grad = 0\n","y.grad = 1\n","y.backward()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ivKVjg0tKQ1n"},"outputs":[],"source":["x.grad"]},{"cell_type":"markdown","metadata":{"id":"SBDfSGfcKQ1p"},"source":["## Apply `Scalar` to linear regression\n","* set the random seed to `0`\n","* randomly init the model parameter `w`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iy4Phpx4KQ1p"},"outputs":[],"source":["pt.manual_seed(0)\n","w = Scalar(pt.randn(1).item())\n","w"]},{"cell_type":"markdown","metadata":{"id":"gIYhatJTKQ1s"},"source":["## Make linear regression data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"txdVmceyKQ1s"},"outputs":[],"source":["ptX = pt.linspace(-5, 5, 100)\n","pty = 5 * ptX + pt.randn(len(ptX))\n","\n","X = [Scalar(x.item()) for x in ptX]\n","y = [Scalar(y.item()) for y in pty]"]},{"cell_type":"markdown","metadata":{"id":"hHnQPRuvKQ1v"},"source":["## Implement a `forward` function using `w`\n","* **hint:** the function should return $ w * X $"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1rD8GNipKQ1v"},"outputs":[],"source":["def forward(w, X):\n","    return [w * X[i] for i in range(len(X))]"]},{"cell_type":"markdown","metadata":{"id":"bHeMP08VKQ1x"},"source":["## Implement the mean squared error calculation\n","* **hint:** Python `sum` can use a starter value as the 2nd argument"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BiW6fdXIKQ1x"},"outputs":[],"source":["def loss(y_pred, y):\n","  error = [y_pred[i] + Scalar(-1.) * y[i] for i in range(len(y))]\n","  squared_error = [error[i] * error[i] for i in range(len(y))]\n","  mean_squared_error = sum(squared_error, Scalar(0)) * Scalar(1.0 / len(y))\n","  return mean_squared_error"]},{"cell_type":"markdown","metadata":{"id":"F5qi1_frKQ1z"},"source":["## Confirm that gradient descent reduces MSE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v9ztA1u7KQ10"},"outputs":[],"source":["LEARNING_RATE = 0.03\n","\n","for _ in range(20):\n","\n","    y_pred = forward(w, X)\n","\n","    mse = loss(y_pred, y)\n","\n","    w.grad = 0.\n","    mse.grad = 1.\n","    mse.backward()\n","\n","    w.val -= LEARNING_RATE * w.grad\n","\n","    print(mse.val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-c14ybHKQ12"},"outputs":[],"source":["w.val"]},{"cell_type":"markdown","metadata":{"id":"2DKGuFd1KQ14"},"source":["## Compare the value of `w` to the analytical solution\n","* the ordinary least squares solution is $ (X^TX)^{-1}X^Ty $\n","\n","\n","* **self-check:** why are neither of the values equal to `5`?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCSn0aTSKQ14"},"outputs":[],"source":["(pt.pow(ptX.T @ ptX, -1) * ptX.T @ pty).item()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":0}